{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3951e0-beab-4863-87d0-4e9dc292c3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "IMPROVED PHISHING DETECTION SYSTEM\n",
      "Learning patterns purely from dataset...\n",
      "============================================================\n",
      "Loading dataset...\n",
      "Initial shape: (200000, 18)\n",
      "Final shape: (60054, 2)\n",
      "Label distribution: {1: 43918, 0: 16136}\n",
      "Learning patterns from data...\n",
      "Phishing URLs: 43918\n",
      "Legitimate URLs: 16136\n",
      "Learned 28 legitimate domains from data\n",
      "Found 13 suspicious characters\n",
      "Found 6 suspicious n-grams\n",
      "Extracting improved features...\n",
      "Feature matrix shape: (60054, 37)\n",
      "Features: ['length', 'length_ratio_phish', 'length_ratio_legit', 'length_above_phish_95th', 'length_very_long', 'length_very_short', 'is_learned_legit_domain', 'tld_suspicion_score', 'tld_very_suspicious', 'subdomain_count', 'domain_length', 'has_dash_in_domain', 'suspicious_char_score', 'suspicious_char_count', 'suspicious_char_ratio', 'high_suspicious_char_ratio', 'ngram_suspicion_score', 'ngram_match_count', 'very_suspicious_ngrams', 'digit_count', 'digit_ratio', 'high_digit_ratio', 'dash_count', 'dot_count', 'slash_count', 'equals_count', 'question_count', 'ampersand_count', 'has_query', 'has_fragment', 'many_params', 'entropy', 'char_diversity', 'high_entropy', 'has_security_words', 'has_brand_imitation', 'suspicious_tld']\n",
      "Training set: (45040, 37), Test set: (15014, 37)\n",
      "Training model...\n",
      "\n",
      "==================================================\n",
      "MODEL EVALUATION\n",
      "==================================================\n",
      "Cross-validation ROC-AUC: 1.0000 (+/- 0.0000)\n",
      "Test Accuracy: 1.0000\n",
      "Test ROC-AUC: 1.0000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Legitimate       1.00      1.00      1.00      4034\n",
      "    Phishing       1.00      1.00      1.00     10980\n",
      "\n",
      "    accuracy                           1.00     15014\n",
      "   macro avg       1.00      1.00      1.00     15014\n",
      "weighted avg       1.00      1.00      1.00     15014\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "True Negatives (Correct Legitimate): 4034\n",
      "False Positives (Legitimate as Phishing): 0\n",
      "False Negatives (Phishing as Legitimate): 0\n",
      "True Positives (Correct Phishing): 10980\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "has_dash_in_domain: 0.1962\n",
      "dash_count: 0.1816\n",
      "suspicious_char_score: 0.1725\n",
      "is_learned_legit_domain: 0.1260\n",
      "domain_length: 0.1128\n",
      "suspicious_char_count: 0.0674\n",
      "suspicious_char_ratio: 0.0340\n",
      "ngram_match_count: 0.0293\n",
      "ngram_suspicion_score: 0.0202\n",
      "char_diversity: 0.0192\n",
      "\n",
      "============================================================\n",
      "INTERACTIVE TESTING\n",
      "Enter URLs to test (type 'exit' to quit):\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "URL:  http://www.bankofamerica-login-update.com\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result: PHISHING (HIGH confidence)\n",
      "Probability: 1.000\n",
      "Reasons:\n",
      "  1. Contains security-related words (common in phishing)\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "URL:  https://amazon-account-update.top\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result: PHISHING (HIGH confidence)\n",
      "Probability: 0.958\n",
      "Reasons:\n",
      "  1. Contains security-related words (common in phishing)\n",
      "  2. Contains brand names (possible imitation)\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "URL:  http://facebook.verify-user-login.xyz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result: PHISHING (HIGH confidence)\n",
      "Probability: 0.967\n",
      "Reasons:\n",
      "  1. Contains security-related words (common in phishing)\n",
      "  2. Contains brand names (possible imitation)\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "URL:  http://secure-login-google.com/signin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result: PHISHING (HIGH confidence)\n",
      "Probability: 0.984\n",
      "Reasons:\n",
      "  1. Contains security-related words (common in phishing)\n",
      "  2. Contains brand names (possible imitation)\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "URL:  https://www.amazon.com/product/B09XYZ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result: LEGITIMATE (HIGH confidence)\n",
      "Probability: 0.003\n",
      "Reasons:\n",
      "  1. Domain recognized as legitimate from training data\n",
      "  2. Contains brand names (possible imitation)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os, re, joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "from collections import Counter, defaultdict\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "DATA_FILE = '/Users/yahyamohnd/Downloads/Phishing_dataset_full_large.csv'\n",
    "TEST_SIZE = 0.25\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# --------------- DATA LOADING ----------------\n",
    "def load_and_prepare_data(file_path):\n",
    "    print(\"Loading dataset...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Initial shape: {df.shape}\")\n",
    "    \n",
    "    # Better column detection\n",
    "    text_col = None\n",
    "    for col in ['text', 'url', 'URL', 'link']:\n",
    "        if col in df.columns:\n",
    "            text_col = col\n",
    "            break\n",
    "    \n",
    "    label_col = None\n",
    "    for col in ['label', 'class', 'target']:\n",
    "        if col in df.columns:\n",
    "            label_col = col\n",
    "            break\n",
    "    \n",
    "    if text_col is None or label_col is None:\n",
    "        print(f\"Available columns: {df.columns.tolist()}\")\n",
    "        raise ValueError(\"Could not find text and label columns\")\n",
    "    \n",
    "    df = df[[text_col, label_col]].rename(columns={text_col: 'text', label_col: 'label'})\n",
    "    df = df.dropna()\n",
    "    df = df[df['text'].str.len() > 5]\n",
    "    df = df.drop_duplicates(subset=['text'])\n",
    "    \n",
    "    print(f\"Final shape: {df.shape}\")\n",
    "    print(f\"Label distribution: {df['label'].value_counts().to_dict()}\")\n",
    "    return df\n",
    "\n",
    "# --------------- IMPROVED PATTERN LEARNING ----------------\n",
    "def learn_patterns_from_data(df):\n",
    "    print(\"Learning patterns from data...\")\n",
    "    phishing_urls = df[df['label']==1]['text'].tolist()\n",
    "    legitimate_urls = df[df['label']==0]['text'].tolist()\n",
    "    \n",
    "    print(f\"Phishing URLs: {len(phishing_urls)}\")\n",
    "    print(f\"Legitimate URLs: {len(legitimate_urls)}\")\n",
    "\n",
    "    patterns = {}\n",
    "    \n",
    "    # 1. Length patterns with better statistics\n",
    "    phish_lengths = [len(u) for u in phishing_urls]\n",
    "    legit_lengths = [len(u) for u in legitimate_urls]\n",
    "    \n",
    "    patterns['length_stats'] = {\n",
    "        'phish_mean': np.mean(phish_lengths),\n",
    "        'legit_mean': np.mean(legit_lengths),\n",
    "        'phish_std': np.std(phish_lengths),\n",
    "        'legit_std': np.std(legit_lengths),\n",
    "        'phish_75th': np.percentile(phish_lengths, 75),\n",
    "        'legit_75th': np.percentile(legit_lengths, 75),\n",
    "        'phish_95th': np.percentile(phish_lengths, 95),\n",
    "        'legit_95th': np.percentile(legit_lengths, 95),\n",
    "    }\n",
    "    \n",
    "    # 2. Domain analysis - learn legitimate vs phishing domains\n",
    "    phish_domains = []\n",
    "    legit_domains = []\n",
    "    \n",
    "    for url in phishing_urls:\n",
    "        try:\n",
    "            if not url.startswith(('http://', 'https://')):\n",
    "                url = 'http://' + url\n",
    "            domain = urlparse(url).netloc.lower()\n",
    "            if domain: phish_domains.append(domain)\n",
    "        except: continue\n",
    "    \n",
    "    for url in legitimate_urls:\n",
    "        try:\n",
    "            if not url.startswith(('http://', 'https://')):\n",
    "                url = 'http://' + url\n",
    "            domain = urlparse(url).netloc.lower()\n",
    "            if domain: legit_domains.append(domain)\n",
    "        except: continue\n",
    "    \n",
    "    # Learn domain patterns (not hardcoded list)\n",
    "    phish_domain_freq = Counter(phish_domains)\n",
    "    legit_domain_freq = Counter(legit_domains)\n",
    "    \n",
    "    # Find domains that appear frequently in legitimate data\n",
    "    learned_legit_domains = set()\n",
    "    for domain, count in legit_domain_freq.most_common(100):\n",
    "        if count >= 5:  # Must appear at least 5 times\n",
    "            phish_count = phish_domain_freq.get(domain, 0)\n",
    "            if phish_count == 0 or count / (phish_count + 1) > 10:  # Strong legitimate signal\n",
    "                learned_legit_domains.add(domain)\n",
    "    \n",
    "    patterns['learned_legit_domains'] = learned_legit_domains\n",
    "    print(f\"Learned {len(learned_legit_domains)} legitimate domains from data\")\n",
    "    \n",
    "    # 3. TLD patterns\n",
    "    phish_tlds = [d.split('.')[-1] for d in phish_domains if '.' in d]\n",
    "    legit_tlds = [d.split('.')[-1] for d in legit_domains if '.' in d]\n",
    "    \n",
    "    phish_tld_freq = Counter(phish_tlds)\n",
    "    legit_tld_freq = Counter(legit_tlds)\n",
    "    \n",
    "    tld_suspicion_scores = {}\n",
    "    for tld in set(phish_tlds + legit_tlds):\n",
    "        phish_count = phish_tld_freq.get(tld, 0)\n",
    "        legit_count = legit_tld_freq.get(tld, 0)\n",
    "        \n",
    "        if phish_count + legit_count >= 10:  # Minimum occurrences\n",
    "            phish_rate = phish_count / len(phish_tlds) if phish_tlds else 0\n",
    "            legit_rate = legit_count / len(legit_tlds) if legit_tlds else 0\n",
    "            \n",
    "            if legit_rate > 0:\n",
    "                suspicion_score = phish_rate / legit_rate\n",
    "                tld_suspicion_scores[tld] = suspicion_score\n",
    "    \n",
    "    patterns['tld_suspicion_scores'] = tld_suspicion_scores\n",
    "    \n",
    "    # 4. Character patterns with better filtering\n",
    "    phish_char_stats = defaultdict(int)\n",
    "    legit_char_stats = defaultdict(int)\n",
    "    \n",
    "    for url in phishing_urls:\n",
    "        for c in url.lower(): phish_char_stats[c] += 1\n",
    "    for url in legitimate_urls:\n",
    "        for c in url.lower(): legit_char_stats[c] += 1\n",
    "    \n",
    "    suspicious_chars = {}\n",
    "    total_phish = sum(phish_char_stats.values())\n",
    "    total_legit = sum(legit_char_stats.values())\n",
    "    \n",
    "    for c in set(phish_char_stats.keys()) | set(legit_char_stats.keys()):\n",
    "        phish_freq = phish_char_stats[c] / total_phish\n",
    "        legit_freq = legit_char_stats.get(c, 1) / total_legit\n",
    "        \n",
    "        # More strict filtering - only characters that are significantly more common in phishing\n",
    "        score = phish_freq / (legit_freq + 1e-10)\n",
    "        if score > 2.0 and phish_char_stats[c] > 50:  # Higher thresholds\n",
    "            suspicious_chars[c] = score\n",
    "    \n",
    "    patterns['suspicious_chars'] = suspicious_chars\n",
    "    print(f\"Found {len(suspicious_chars)} suspicious characters\")\n",
    "    \n",
    "    # 5. N-gram patterns with better filtering\n",
    "    def extract_ngrams(urls, n_range=(3, 6), max_samples=2000):\n",
    "        ngrams = []\n",
    "        for url in urls[:max_samples]:\n",
    "            u = re.sub(r'https?://', '', url.lower())\n",
    "            u = re.sub(r'[^a-z0-9.-]', '', u)\n",
    "            for n in range(n_range[0], min(n_range[1] + 1, len(u))):\n",
    "                for i in range(len(u) - n + 1):\n",
    "                    ngram = u[i:i+n]\n",
    "                    if len(set(ngram)) > 1:  # Not repetitive\n",
    "                        ngrams.append(ngram)\n",
    "        return ngrams\n",
    "    \n",
    "    phish_ngrams = extract_ngrams(phishing_urls)\n",
    "    legit_ngrams = extract_ngrams(legitimate_urls)\n",
    "    \n",
    "    phish_counts = Counter(phish_ngrams)\n",
    "    legit_counts = Counter(legit_ngrams)\n",
    "    \n",
    "    suspicious_ngrams = {}\n",
    "    for ng, cnt in phish_counts.most_common(500):\n",
    "        if cnt >= 10:  # Higher minimum frequency\n",
    "            legit_cnt = legit_counts.get(ng, 0)\n",
    "            phish_rate = cnt / len(phish_ngrams)\n",
    "            legit_rate = legit_cnt / len(legit_ngrams) if legit_ngrams else 0\n",
    "            \n",
    "            if legit_rate == 0:\n",
    "                score = phish_rate * 1000  # Very suspicious if only in phishing\n",
    "            else:\n",
    "                score = phish_rate / legit_rate\n",
    "            \n",
    "            # Much higher threshold for n-grams\n",
    "            if score >= 5.0:  # At least 5x more common in phishing\n",
    "                suspicious_ngrams[ng] = {\n",
    "                    'score': score,\n",
    "                    'phish_count': cnt,\n",
    "                    'legit_count': legit_cnt\n",
    "                }\n",
    "    \n",
    "    patterns['suspicious_ngrams'] = suspicious_ngrams\n",
    "    print(f\"Found {len(suspicious_ngrams)} suspicious n-grams\")\n",
    "    \n",
    "    return patterns\n",
    "\n",
    "# --------------- IMPROVED FEATURE EXTRACTION ----------------\n",
    "def extract_improved_features(text, learned_patterns):\n",
    "    features = {}\n",
    "    t = text.lower()\n",
    "    \n",
    "    # 1. Length features with learned thresholds\n",
    "    features['length'] = len(text)\n",
    "    length_stats = learned_patterns.get('length_stats', {})\n",
    "    if length_stats:\n",
    "        # More sophisticated length features\n",
    "        phish_mean = length_stats.get('phish_mean', 50)\n",
    "        legit_mean = length_stats.get('legit_mean', 50)\n",
    "        \n",
    "        features['length_ratio_phish'] = len(text) / max(phish_mean, 10)\n",
    "        features['length_ratio_legit'] = len(text) / max(legit_mean, 10)\n",
    "        features['length_above_phish_95th'] = 1 if len(text) > length_stats.get('phish_95th', 200) else 0\n",
    "        features['length_very_long'] = 1 if len(text) > 150 else 0\n",
    "        features['length_very_short'] = 1 if len(text) < 20 else 0\n",
    "    \n",
    "    # 2. Domain legitimacy check (learned from data)\n",
    "    try:\n",
    "        if not text.startswith(('http://', 'https://')):\n",
    "            text_parsed = 'http://' + text\n",
    "        else:\n",
    "            text_parsed = text\n",
    "        \n",
    "        parsed = urlparse(text_parsed)\n",
    "        domain = parsed.netloc.lower() if parsed.netloc else ''\n",
    "        \n",
    "        learned_legit_domains = learned_patterns.get('learned_legit_domains', set())\n",
    "        features['is_learned_legit_domain'] = 1 if domain in learned_legit_domains else 0\n",
    "        \n",
    "        # TLD suspicion based on learned patterns\n",
    "        tld_scores = learned_patterns.get('tld_suspicion_scores', {})\n",
    "        if '.' in domain:\n",
    "            tld = domain.split('.')[-1]\n",
    "            features['tld_suspicion_score'] = tld_scores.get(tld, 1.0)\n",
    "            features['tld_very_suspicious'] = 1 if tld_scores.get(tld, 1.0) > 3.0 else 0\n",
    "        else:\n",
    "            features['tld_suspicion_score'] = 1.0\n",
    "            features['tld_very_suspicious'] = 0\n",
    "        \n",
    "        # Domain structure\n",
    "        features['subdomain_count'] = max(0, domain.count('.') - 1) if domain else 0\n",
    "        features['domain_length'] = len(domain)\n",
    "        features['has_dash_in_domain'] = 1 if '-' in domain else 0\n",
    "        \n",
    "    except:\n",
    "        features.update({\n",
    "            'is_learned_legit_domain': 0, 'tld_suspicion_score': 1.0,\n",
    "            'tld_very_suspicious': 0, 'subdomain_count': 0,\n",
    "            'domain_length': 0, 'has_dash_in_domain': 0\n",
    "        })\n",
    "    \n",
    "    # 3. Improved character analysis\n",
    "    suspicious_chars = learned_patterns.get('suspicious_chars', {})\n",
    "    char_score = 0\n",
    "    char_count = 0\n",
    "    \n",
    "    for c, score in suspicious_chars.items():\n",
    "        count = t.count(c)\n",
    "        if count > 0:\n",
    "            char_score += count * score\n",
    "            char_count += count\n",
    "    \n",
    "    features['suspicious_char_score'] = char_score\n",
    "    features['suspicious_char_count'] = char_count\n",
    "    \n",
    "    # Better normalization for char ratio\n",
    "    total_chars = len([c for c in text if c.isalnum() or c in '.-/'])\n",
    "    features['suspicious_char_ratio'] = char_count / max(total_chars, 1)\n",
    "    features['high_suspicious_char_ratio'] = 1 if features['suspicious_char_ratio'] > 0.3 else 0\n",
    "    \n",
    "    # 4. Improved n-gram analysis\n",
    "    suspicious_ngrams = learned_patterns.get('suspicious_ngrams', {})\n",
    "    t_clean = re.sub(r'https?://', '', t)\n",
    "    t_clean = re.sub(r'[^a-z0-9.-]', '', t_clean)\n",
    "    \n",
    "    ngram_score = 0\n",
    "    ngram_count = 0\n",
    "    high_score_ngrams = 0\n",
    "    \n",
    "    for ng, info in suspicious_ngrams.items():\n",
    "        if ng in t_clean:\n",
    "            score = info['score']\n",
    "            ngram_score += score\n",
    "            ngram_count += 1\n",
    "            if score > 20:  # Very high threshold\n",
    "                high_score_ngrams += 1\n",
    "    \n",
    "    features['ngram_suspicion_score'] = ngram_score\n",
    "    features['ngram_match_count'] = ngram_count\n",
    "    features['very_suspicious_ngrams'] = high_score_ngrams\n",
    "    \n",
    "    # 5. URL structure features\n",
    "    features['digit_count'] = sum(c.isdigit() for c in text)\n",
    "    features['digit_ratio'] = features['digit_count'] / max(1, len(text))\n",
    "    features['high_digit_ratio'] = 1 if features['digit_ratio'] > 0.3 else 0\n",
    "    \n",
    "    # Special characters with better analysis\n",
    "    features['dash_count'] = text.count('-')\n",
    "    features['dot_count'] = text.count('.')\n",
    "    features['slash_count'] = text.count('/')\n",
    "    features['equals_count'] = text.count('=')\n",
    "    features['question_count'] = text.count('?')\n",
    "    features['ampersand_count'] = text.count('&')\n",
    "    \n",
    "    # URL structure indicators\n",
    "    features['has_query'] = 1 if '?' in text else 0\n",
    "    features['has_fragment'] = 1 if '#' in text else 0\n",
    "    features['many_params'] = 1 if text.count('=') > 3 else 0\n",
    "    \n",
    "    # 6. Entropy calculation (but normalized better)\n",
    "    if t_clean and len(t_clean) > 0:\n",
    "        counts = {}\n",
    "        for c in t_clean:\n",
    "            counts[c] = counts.get(c, 0) + 1\n",
    "        \n",
    "        entropy = 0\n",
    "        for v in counts.values():\n",
    "            p = v / len(t_clean)\n",
    "            entropy += -p * np.log2(p)\n",
    "        \n",
    "        features['entropy'] = entropy\n",
    "        features['char_diversity'] = len(counts) / len(t_clean)\n",
    "        features['high_entropy'] = 1 if entropy > 4.5 else 0  # Higher threshold\n",
    "    else:\n",
    "        features['entropy'] = 0\n",
    "        features['char_diversity'] = 0\n",
    "        features['high_entropy'] = 0\n",
    "    \n",
    "    # 7. Phishing-specific patterns\n",
    "    features['has_security_words'] = 1 if any(word in t for word in ['secure', 'verify', 'update', 'login', 'account']) else 0\n",
    "    features['has_brand_imitation'] = 1 if any(brand in t for brand in ['paypal', 'apple', 'microsoft', 'google', 'amazon', 'facebook']) else 0\n",
    "    features['suspicious_tld'] = 1 if any(tld in text for tld in ['.tk', '.ml', '.ga', '.cf', '.info', '.biz']) else 0\n",
    "    \n",
    "    return features\n",
    "\n",
    "# --------------- MODEL TRAINING WITH EVALUATION ----------------\n",
    "def train_improved_model(df, learned_patterns):\n",
    "    print(\"Extracting improved features...\")\n",
    "    feature_data = [extract_improved_features(text, learned_patterns) for text in df['text'].values]\n",
    "    features_df = pd.DataFrame(feature_data).fillna(0)\n",
    "    \n",
    "    print(f\"Feature matrix shape: {features_df.shape}\")\n",
    "    print(\"Features:\", list(features_df.columns))\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = RobustScaler()\n",
    "    X_scaled = scaler.fit_transform(features_df.values)\n",
    "    X = csr_matrix(X_scaled)\n",
    "    y = df['label'].values\n",
    "    \n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=TEST_SIZE, stratify=y, random_state=RANDOM_STATE\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set: {X_train.shape}, Test set: {X_test.shape}\")\n",
    "    \n",
    "    # Train model\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=15,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        random_state=RANDOM_STATE,\n",
    "        class_weight='balanced'  # Handle class imbalance\n",
    "    )\n",
    "    \n",
    "    print(\"Training model...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate model\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"MODEL EVALUATION\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "    print(f\"Cross-validation ROC-AUC: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "    \n",
    "    # Test set evaluation\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    print(f\"Test Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"Test ROC-AUC: {roc_auc_score(y_test, y_prob):.4f}\")\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['Legitimate', 'Phishing']))\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(f\"True Negatives (Correct Legitimate): {tn}\")\n",
    "    print(f\"False Positives (Legitimate as Phishing): {fp}\")  \n",
    "    print(f\"False Negatives (Phishing as Legitimate): {fn}\")\n",
    "    print(f\"True Positives (Correct Phishing): {tp}\")\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': features_df.columns,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\nTop 10 Most Important Features:\")\n",
    "    for i, row in feature_importance.head(10).iterrows():\n",
    "        print(f\"{row['feature']}: {row['importance']:.4f}\")\n",
    "    \n",
    "    return model, scaler, features_df.columns.tolist(), feature_importance\n",
    "\n",
    "# --------------- BEHAVIOR ANALYSIS ----------------\n",
    "def analyze_model_behavior(model, scaler, feature_columns, learned_patterns, df):\n",
    "    \"\"\"Analyze how the model behaves on different types of URLs\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MODEL BEHAVIOR ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Extract features for all URLs in dataset\n",
    "    all_features = []\n",
    "    for text in df['text'].values:\n",
    "        features = extract_improved_features(text, learned_patterns)\n",
    "        all_features.append(features)\n",
    "    \n",
    "    features_df = pd.DataFrame(all_features).fillna(0)\n",
    "    features_df = features_df.reindex(columns=feature_columns, fill_value=0)\n",
    "    \n",
    "    X_scaled = scaler.transform(features_df.values)\n",
    "    probabilities = model.predict_proba(X_scaled)[:, 1]\n",
    "    \n",
    "    # Add predictions to dataframe for analysis\n",
    "    analysis_df = df.copy()\n",
    "    analysis_df['predicted_prob'] = probabilities\n",
    "    analysis_df['predicted_label'] = (probabilities >= 0.5).astype(int)\n",
    "    analysis_df['correct'] = (analysis_df['predicted_label'] == analysis_df['label'])\n",
    "    \n",
    "    # 1. Confidence distribution analysis\n",
    "    print(\"\\n1. CONFIDENCE DISTRIBUTION:\")\n",
    "    high_conf = sum(1 for p in probabilities if p > 0.8 or p < 0.2)\n",
    "    medium_conf = sum(1 for p in probabilities if 0.2 <= p <= 0.8)\n",
    "    print(f\"High confidence predictions (>0.8 or <0.2): {high_conf} ({high_conf/len(probabilities)*100:.1f}%)\")\n",
    "    print(f\"Medium confidence predictions (0.2-0.8): {medium_conf} ({medium_conf/len(probabilities)*100:.1f}%)\")\n",
    "    \n",
    "    # 2. Error analysis\n",
    "    print(\"\\n2. ERROR ANALYSIS:\")\n",
    "    false_positives = analysis_df[(analysis_df['label'] == 0) & (analysis_df['predicted_label'] == 1)]\n",
    "    false_negatives = analysis_df[(analysis_df['label'] == 1) & (analysis_df['predicted_label'] == 0)]\n",
    "    \n",
    "    print(f\"False Positives (Legitimate predicted as Phishing): {len(false_positives)}\")\n",
    "    print(f\"False Negatives (Phishing predicted as Legitimate): {len(false_negatives)}\")\n",
    "    \n",
    "    if len(false_positives) > 0:\n",
    "        print(\"\\nSample False Positives:\")\n",
    "        for i, (idx, row) in enumerate(false_positives.head(3).iterrows()):\n",
    "            print(f\"  {i+1}. {row['text']} (prob: {row['predicted_prob']:.3f})\")\n",
    "    \n",
    "    if len(false_negatives) > 0:\n",
    "        print(\"\\nSample False Negatives:\")\n",
    "        for i, (idx, row) in enumerate(false_negatives.head(3).iterrows()):\n",
    "            print(f\"  {i+1}. {row['text']} (prob: {row['predicted_prob']:.3f})\")\n",
    "    \n",
    "    # 3. Feature impact analysis\n",
    "    print(\"\\n3. FEATURE IMPACT ON DIFFERENT URL TYPES:\")\n",
    "    \n",
    "    # Analyze feature values for different groups\n",
    "    legit_features = features_df[analysis_df['label'] == 0]\n",
    "    phish_features = features_df[analysis_df['label'] == 1]\n",
    "    \n",
    "    print(\"\\nAverage feature values:\")\n",
    "    print(f\"{'Feature':<25} {'Legitimate':<12} {'Phishing':<12} {'Difference':<12}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    for col in feature_columns[:10]:  # Top 10 features\n",
    "        legit_mean = legit_features[col].mean()\n",
    "        phish_mean = phish_features[col].mean()\n",
    "        diff = phish_mean - legit_mean\n",
    "        print(f\"{col:<25} {legit_mean:<12.3f} {phish_mean:<12.3f} {diff:<12.3f}\")\n",
    "    \n",
    "    # 4. Length behavior analysis\n",
    "    print(\"\\n4. URL LENGTH BEHAVIOR:\")\n",
    "    legit_lengths = [len(url) for url in analysis_df[analysis_df['label'] == 0]['text']]\n",
    "    phish_lengths = [len(url) for url in analysis_df[analysis_df['label'] == 1]['text']]\n",
    "    \n",
    "    print(f\"Legitimate URLs - Mean: {np.mean(legit_lengths):.1f}, Std: {np.std(legit_lengths):.1f}\")\n",
    "    print(f\"Phishing URLs - Mean: {np.mean(phish_lengths):.1f}, Std: {np.std(phish_lengths):.1f}\")\n",
    "    \n",
    "    # Length-based accuracy\n",
    "    short_urls = analysis_df[analysis_df['text'].str.len() < 50]\n",
    "    medium_urls = analysis_df[(analysis_df['text'].str.len() >= 50) & (analysis_df['text'].str.len() < 100)]\n",
    "    long_urls = analysis_df[analysis_df['text'].str.len() >= 100]\n",
    "    \n",
    "    if len(short_urls) > 0:\n",
    "        short_acc = short_urls['correct'].mean()\n",
    "        print(f\"Accuracy on short URLs (<50 chars): {short_acc:.3f} ({len(short_urls)} samples)\")\n",
    "    \n",
    "    if len(medium_urls) > 0:\n",
    "        medium_acc = medium_urls['correct'].mean()\n",
    "        print(f\"Accuracy on medium URLs (50-100 chars): {medium_acc:.3f} ({len(medium_urls)} samples)\")\n",
    "    \n",
    "    if len(long_urls) > 0:\n",
    "        long_acc = long_urls['correct'].mean()\n",
    "        print(f\"Accuracy on long URLs (>100 chars): {long_acc:.3f} ({len(long_urls)} samples)\")\n",
    "    \n",
    "    # 5. Domain behavior analysis\n",
    "    print(\"\\n5. DOMAIN-BASED BEHAVIOR:\")\n",
    "    learned_legit_domains = learned_patterns.get('learned_legit_domains', set())\n",
    "    \n",
    "    # URLs with learned legitimate domains\n",
    "    legit_domain_mask = []\n",
    "    for url in analysis_df['text']:\n",
    "        try:\n",
    "            if not url.startswith(('http://', 'https://')):\n",
    "                url = 'http://' + url\n",
    "            domain = urlparse(url).netloc.lower()\n",
    "            legit_domain_mask.append(domain in learned_legit_domains)\n",
    "        except:\n",
    "            legit_domain_mask.append(False)\n",
    "    \n",
    "    analysis_df['has_learned_legit_domain'] = legit_domain_mask\n",
    "    legit_domain_urls = analysis_df[analysis_df['has_learned_legit_domain']]\n",
    "    \n",
    "    if len(legit_domain_urls) > 0:\n",
    "        legit_domain_acc = legit_domain_urls['correct'].mean()\n",
    "        print(f\"Accuracy on URLs with learned legitimate domains: {legit_domain_acc:.3f} ({len(legit_domain_urls)} samples)\")\n",
    "        \n",
    "        # How often are learned legitimate domains correctly classified as legitimate?\n",
    "        true_legit_with_legit_domain = legit_domain_urls[legit_domain_urls['label'] == 0]\n",
    "        if len(true_legit_with_legit_domain) > 0:\n",
    "            legit_recognition_rate = (true_legit_with_legit_domain['predicted_label'] == 0).mean()\n",
    "            print(f\"Recognition rate for legitimate URLs with learned domains: {legit_recognition_rate:.3f}\")\n",
    "    \n",
    "    # 6. Pattern matching behavior\n",
    "    print(\"\\n6. PATTERN MATCHING BEHAVIOR:\")\n",
    "    suspicious_ngrams = learned_patterns.get('suspicious_ngrams', {})\n",
    "    \n",
    "    if suspicious_ngrams:\n",
    "        pattern_matches = []\n",
    "        for url in analysis_df['text']:\n",
    "            url_clean = re.sub(r'https?://', '', url.lower())\n",
    "            url_clean = re.sub(r'[^a-z0-9.-]', '', url_clean)\n",
    "            \n",
    "            match_count = 0\n",
    "            for pattern in suspicious_ngrams.keys():\n",
    "                if pattern in url_clean:\n",
    "                    match_count += 1\n",
    "            pattern_matches.append(match_count)\n",
    "        \n",
    "        analysis_df['pattern_matches'] = pattern_matches\n",
    "        \n",
    "        # URLs with many pattern matches\n",
    "        high_pattern_urls = analysis_df[analysis_df['pattern_matches'] > 5]\n",
    "        if len(high_pattern_urls) > 0:\n",
    "            high_pattern_acc = high_pattern_urls['correct'].mean()\n",
    "            print(f\"Accuracy on URLs with >5 suspicious patterns: {high_pattern_acc:.3f} ({len(high_pattern_urls)} samples)\")\n",
    "        \n",
    "        # URLs with no pattern matches\n",
    "        no_pattern_urls = analysis_df[analysis_df['pattern_matches'] == 0]\n",
    "        if len(no_pattern_urls) > 0:\n",
    "            no_pattern_acc = no_pattern_urls['correct'].mean()\n",
    "            print(f\"Accuracy on URLs with no suspicious patterns: {no_pattern_acc:.3f} ({len(no_pattern_urls)} samples)\")\n",
    "    \n",
    "    # 7. Probability threshold analysis\n",
    "    print(\"\\n7. THRESHOLD SENSITIVITY ANALYSIS:\")\n",
    "    thresholds = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "    \n",
    "    print(f\"{'Threshold':<12} {'Accuracy':<12} {'Precision':<12} {'Recall':<12}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        pred_at_threshold = (probabilities >= threshold).astype(int)\n",
    "        accuracy = accuracy_score(analysis_df['label'], pred_at_threshold)\n",
    "        \n",
    "        tp = sum((pred_at_threshold == 1) & (analysis_df['label'] == 1))\n",
    "        fp = sum((pred_at_threshold == 1) & (analysis_df['label'] == 0))\n",
    "        fn = sum((pred_at_threshold == 0) & (analysis_df['label'] == 1))\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        \n",
    "        print(f\"{threshold:<12} {accuracy:<12.3f} {precision:<12.3f} {recall:<12.3f}\")\n",
    "    \n",
    "    return analysis_df\n",
    "\n",
    "# --------------- IMPROVED PREDICTION ----------------\n",
    "def predict_url_improved(url, model, scaler, feature_columns, learned_patterns):\n",
    "    features = extract_improved_features(url, learned_patterns)\n",
    "    \n",
    "    # Convert to DataFrame to ensure correct column order\n",
    "    features_df = pd.DataFrame([features])\n",
    "    features_df = features_df.reindex(columns=feature_columns, fill_value=0)\n",
    "    \n",
    "    X_scaled = scaler.transform(features_df.values)\n",
    "    y_prob = model.predict_proba(X_scaled)[0, 1]\n",
    "    y_pred = 1 if y_prob >= 0.5 else 0\n",
    "    \n",
    "    # Generate more intelligent reasons\n",
    "    reasons = []\n",
    "    \n",
    "    if features.get('is_learned_legit_domain', 0) == 1:\n",
    "        reasons.append(\"Domain recognized as legitimate from training data\")\n",
    "    \n",
    "    if features.get('tld_very_suspicious', 0) == 1:\n",
    "        reasons.append(f\"TLD has high suspicion score: {features.get('tld_suspicion_score', 0):.2f}\")\n",
    "    \n",
    "    if features.get('very_suspicious_ngrams', 0) > 0:\n",
    "        reasons.append(f\"{features.get('very_suspicious_ngrams', 0)} very suspicious patterns detected\")\n",
    "    elif features.get('ngram_match_count', 0) > 5:\n",
    "        reasons.append(f\"{features.get('ngram_match_count', 0)} suspicious patterns found\")\n",
    "    \n",
    "    if features.get('high_suspicious_char_ratio', 0) == 1:\n",
    "        reasons.append(f\"High suspicious character ratio: {features.get('suspicious_char_ratio', 0):.2f}\")\n",
    "    \n",
    "    if features.get('length_very_long', 0) == 1:\n",
    "        reasons.append(f\"Unusually long URL ({features.get('length', 0)} chars)\")\n",
    "    \n",
    "    if features.get('has_security_words', 0) == 1:\n",
    "        reasons.append(\"Contains security-related words (common in phishing)\")\n",
    "    \n",
    "    if features.get('has_brand_imitation', 0) == 1:\n",
    "        reasons.append(\"Contains brand names (possible imitation)\")\n",
    "    \n",
    "    if features.get('many_params', 0) == 1:\n",
    "        reasons.append(\"Many parameters in URL\")\n",
    "    \n",
    "    return {\n",
    "        'url': url,\n",
    "        'prediction': 'PHISHING' if y_pred == 1 else 'LEGITIMATE',\n",
    "        'probability': y_prob,\n",
    "        'confidence': 'HIGH' if abs(y_prob - 0.5) > 0.3 else 'MEDIUM' if abs(y_prob - 0.5) > 0.1 else 'LOW',\n",
    "        'reasons': reasons if reasons else ['Based on learned patterns from training data']\n",
    "    }\n",
    "\n",
    "# --------------- MAIN ----------------\n",
    "def main():\n",
    "    try:\n",
    "        print(\"=\"*60)\n",
    "        print(\"IMPROVED PHISHING DETECTION SYSTEM\")\n",
    "        print(\"Learning patterns purely from dataset...\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Load data\n",
    "        df = load_and_prepare_data(DATA_FILE)\n",
    "        \n",
    "        # Learn patterns\n",
    "        learned_patterns = learn_patterns_from_data(df)\n",
    "        \n",
    "        # Train model\n",
    "        model, scaler, feature_columns, feature_importance = train_improved_model(df, learned_patterns)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"INTERACTIVE TESTING\")\n",
    "        print(\"Enter URLs to test (type 'exit' to quit):\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        while True:\n",
    "            user_url = input(\"\\nURL: \").strip()\n",
    "            if user_url.lower() == \"exit\":\n",
    "                break\n",
    "            \n",
    "            if not user_url:\n",
    "                continue\n",
    "                \n",
    "            result = predict_url_improved(user_url, model, scaler, feature_columns, learned_patterns)\n",
    "            print(f\"\\nResult: {result['prediction']} ({result['confidence']} confidence)\")\n",
    "            print(f\"Probability: {result['probability']:.3f}\")\n",
    "            print(\"Reasons:\")\n",
    "            for i, reason in enumerate(result['reasons'], 1):\n",
    "                print(f\"  {i}. {reason}\")\n",
    "            print(\"-\" * 50)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
